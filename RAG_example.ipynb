{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_data(file_path):\n",
    "    # Creating a PyMuPDFLoader object with file_path\n",
    "    loader = PyMuPDFLoader(file_path=file_path)\n",
    "    \n",
    "    # loading the PDF file\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # returning the loaded document\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responsible for splitting the documents into several chunks\n",
    "def split_docs(documents, chunk_size=1024, chunk_overlap=20):\n",
    "    \n",
    "    # Initializing the RecursiveCharacterTextSplitter with\n",
    "    # chunk_size and chunk_overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    # Splitting the documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    \n",
    "    # returning the document chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading the embedding model\n",
    "def load_embedding_model(model_path, normalize_embedding=True):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs={'device':'cpu'}, # here we will run the model with CPU only\n",
    "        encode_kwargs = {\n",
    "            'normalize_embeddings': normalize_embedding # keep True to compute cosine similarity\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Function for creating embeddings using FAISS\n",
    "def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n",
    "    # Creating the embeddings using FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    # Saving the model in current directory\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    # returning the vectorstore\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### System:\n",
    "You are an AI Assistant that follows instructions extreamly well. \\\n",
    "Help as much as you can.\n",
    "Please say I don't know if you don't know.\n",
    "\n",
    "### User:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### System:\n",
    "You are an respectful and honest assistant. You have to answer the user's \\\n",
    "questions using only the context provided to you. If you don't know the answer, \\\n",
    "just say you don't know. Don't try to make up an answer.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### User:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_chain(retriever, llm, prompt):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever, # here we are using the vectorstore as a retriever\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True, # including source documents in output\n",
    "        chain_type_kwargs={'prompt': prompt} # customizing the prompt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query, chain):\n",
    "    # Getting response from chain\n",
    "    response = chain({'query': query})\n",
    "    \n",
    "    # Wrapping the text for better output in Jupyter Notebook\n",
    "    wrapped_text = textwrap.fill(response['result'], width=100)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choisoonwook/anaconda3/envs/ollama_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Loading llama3:70b from Ollama\n",
    "llm = Ollama(model=\"llama3:70b\", temperature=0)\n",
    "\n",
    "# Loading the Embedding Model\n",
    "embed = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and splitting the documents\n",
    "docs = load_pdf_data(file_path=\"data/choisoonwook.pdf\")\n",
    "documents = split_docs(documents=docs)\n",
    "\n",
    "# creating vectorstore\n",
    "vectorstore = create_embeddings(documents, embed)\n",
    "\n",
    "# converting vectorstore to a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the prompt from the template which we created before\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Creating the chain\n",
    "chain = load_qa_chain(retriever, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choisoonwook/anaconda3/envs/ollama_rag/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document appears to be a personal profile or CV of Soon-Wook Choi, a bachelor student at Korea\n",
      "University. The profile includes:  1. Conferences: Two poster presentations in 2023 on information\n",
      "security and cryptography. 2. Education: Bachelor student at Korea University, Department of AI\n",
      "Cyber Security, with a GPA of 3.68/4.5. 3. Research Experiences: Research student at Artificial\n",
      "Intelligence Cyber Security, Korea University Sejong, Korea since March 2021. 4. Research Interests:\n",
      "Quantum machine learning, post-quantum cryptography, binary code analysis, network architecture, and\n",
      "network security. 5. Projects: Three ongoing research projects on quantum computing, cryptography,\n",
      "and AI-based binary similarity analysis. 6. Skills and Techniques: Proficient in using Github for\n",
      "version management, Python for CNN code implementation, and quantum circuit implementation.\n",
      "Overall, the document highlights Soon-Wook Choi's academic background, research experiences, and\n",
      "skills in the field of AI cyber security and cryptography.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"Summarize the document\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what QAOA is. The context provided does not mention QAOA. It seems to be a personal\n",
      "profile of SoonWook Choi, and it discusses his education, research experiences, projects, skills,\n",
      "and conferences related to AI cyber security, quantum machine learning, and post-quantum\n",
      "cryptography.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is QAOA\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what BERT is based on the provided context. The context only provides information about\n",
      "conferences, education, research experiences, research interests, projects, and skills and\n",
      "techniques related to SoonWook Choi, but it does not mention BERT.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is BERT\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soon-Wook Choi is a Bachelor student at Korea University, Department of AI Cyber Security.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"Who is SoonWook Choi\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp1 = llm.invoke(\"What is QAOA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAOA stands for Quantum Approximate Optimization Algorithm, (pronounced \"kuh-oh-ah\"). It's a quantum algorithm designed to solve complex optimization problems more efficiently than classical algorithms.\n",
      "\n",
      "**The Problem:**\n",
      "Optimization problems involve finding the best solution among many possible solutions. These problems are ubiquitous in various fields, as they can model real-world scenarios like:\n",
      "\n",
      "* Scheduling tasks\n",
      "* Allocating resources\n",
      "* Portfolio optimization (finance)\n",
      "* Logistics and supply chain management\n",
      "\n",
      "Classical computers struggle to solve these problems efficiently due to their inherent complexity.\n",
      "\n",
      "**The Solution:**\n",
      "QAOA is a hybrid quantum-classical algorithm that leverages the power of quantum computing to approximate optimal solutions. It's designed to tackle complex optimization problems by exploiting the principles of quantum mechanics, such as superposition and entanglement.\n",
      "\n",
      "Here's a high-level overview of how QAOA works:\n",
      "\n",
      "1. **Problem Formulation**: Define the optimization problem using a cost function (or objective function) that needs to be minimized or maximized.\n",
      "2. **Quantum Circuit**: Create a quantum circuit that encodes the problem into a quantum state. This involves applying a series of quantum gates to a set of qubits (quantum bits).\n",
      "3. **Variational Quantum Eigensolver (VQE)**: Use VQE, a hybrid quantum-classical algorithm, to approximate the optimal solution. VQE iteratively updates the quantum circuit parameters to minimize the cost function.\n",
      "4. **Classical Post-processing**: Perform classical post-processing on the output of the quantum circuit to refine the solution.\n",
      "\n",
      "**Key Benefits:**\n",
      "\n",
      "* QAOA can solve complex optimization problems more efficiently than classical algorithms, potentially leading to breakthroughs in various fields.\n",
      "* It's a hybrid algorithm, which means it can be implemented using existing quantum computing hardware and software.\n",
      "* QAOA is relatively easy to implement compared to other quantum algorithms, making it an attractive option for researchers and developers.\n",
      "\n",
      "**Challenges and Limitations:**\n",
      "\n",
      "* QAOA is still an approximate optimization algorithm, meaning it may not always find the global optimum.\n",
      "* The algorithm's performance depends on the quality of the quantum circuit and the classical post-processing techniques used.\n",
      "* Currently, QAOA is limited by the noise and error rates in existing quantum computing hardware.\n",
      "\n",
      "Despite these challenges, QAOA has shown promising results in various applications, including machine learning, logistics, and finance. As quantum computing continues to evolve, we can expect QAOA to play an increasingly important role in solving complex optimization problems.\n"
     ]
    }
   ],
   "source": [
    "print(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google in  , which has revolutionized the field of Natural Language Processing (NLP).\n",
      "\n",
      "**What does BERT do?**\n",
      "\n",
      "BERT is designed to understand natural language processing tasks, such as:\n",
      "\n",
      "1. **Language Modeling**: Predicting the next word in a sentence based on the context.\n",
      "2. **Text Classification**: Classifying text into categories like spam/not spam, positive/negative sentiment, etc.\n",
      "3. **Named Entity Recognition**: Identifying named entities (people, organizations, locations) in text.\n",
      "4. **Question Answering**: Answering questions based on the content of a passage.\n",
      "\n",
      "**How does BERT work?**\n",
      "\n",
      "BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence. These representations are then fine-tuned for specific NLP tasks.\n",
      "\n",
      "Here's a high-level overview of the architecture:\n",
      "\n",
      "1. **Input Embeddings**: Word embeddings (e.g., WordPiece) and positional embeddings are combined to create input embeddings.\n",
      "2. **Encoder**: A multi-layer bidirectional transformer encoder processes the input embeddings, generating contextualized representations of each word.\n",
      "3. **Pooler**: The final representation of the [CLS] token (a special token added at the beginning of each sentence) is used as the aggregate sequence representation.\n",
      "\n",
      "**What makes BERT so powerful?**\n",
      "\n",
      "1. **Pre-training**: BERT is pre-trained on a large corpus of text data (e.g., Wikipedia, BookCorpus) using a masked language modeling objective. This allows it to learn general language understanding capabilities.\n",
      "2. **Transfer Learning**: Fine-tuning the pre-trained BERT model on specific NLP tasks achieves state-of-the-art results with minimal additional training data and compute resources.\n",
      "3. **Contextualized Representations**: BERT's contextualized representations capture subtle nuances in language, enabling it to understand complex sentences and relationships.\n",
      "\n",
      "**Impact of BERT**\n",
      "\n",
      "BERT has achieved state-of-the-art results on a wide range of NLP tasks, including:\n",
      "\n",
      "1. GLUE benchmark (General Language Understanding Evaluation)\n",
      "2. SQuAD question answering dataset\n",
      "3. IMDB sentiment analysis dataset\n",
      "\n",
      "The success of BERT has led to the development of many variants and extensions, such as RoBERTa, DistilBERT, and ALBERT, which have further improved performance on various NLP tasks.\n",
      "\n",
      "In summary, BERT is a powerful language model that has revolutionized the field of NLP by providing a pre-trained model that can be fine-tuned for specific tasks, achieving state-of-the-art results with minimal additional training data and compute resources.\n"
     ]
    }
   ],
   "source": [
    "resp2 = llm.invoke(\"What is BERT\")\n",
    "print(resp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soon-Wook Choi is a South Korean professional golfer who has had a successful career in the sport. Here are some key facts about him:\n",
      "\n",
      "**Early Life and Amateur Career**: Born on June  , 1963, in Seoul, South Korea, Choi started playing golf at the age of 12. He won several amateur tournaments in his home country before turning pro in 1986.\n",
      "\n",
      "**Professional Career**: Choi has had a prolific career, winning over 20 professional tournaments worldwide, including multiple victories on the Asian Tour, Japanese Tour, and Korean Tour. His most notable win came at the 2003 Korea Open, a co-sanctioned event with the European Tour.\n",
      "\n",
      "**International Appearances**: Choi has represented South Korea in several international team competitions, including the Presidents Cup (1996, 2000), the Dunhill Cup (1989-1991), and the World Cup (1989-1992).\n",
      "\n",
      "**Achievements and Honors**: Choi was named the Asian Tour's Rookie of the Year in 1987 and Player of the Year in 1994. He has also been recognized for his contributions to Korean golf, receiving the Korean Golf Association's highest honor, the \"Golf Merit Award,\" in 2005.\n",
      "\n",
      "**Legacy**: Soon-Wook Choi is considered one of the pioneers of Korean professional golf, paving the way for future generations of Korean golfers. His achievements have inspired many young players in his home country to pursue careers in the sport.\n",
      "\n",
      "Would you like to know more about his career statistics or accomplishments?\n"
     ]
    }
   ],
   "source": [
    "resp3 = llm.invoke(\"Who is SoonWook Choi\")\n",
    "print(resp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "A Tutorial on Quantum Approximate Optimization Algorithm QAOA Fundamentals and Applications.pdf\n",
      "self_rag_arxiv.pdf\n",
      "BERT_arxiv.pdf\n",
      "crag_arxiv.pdf\n",
      "RAG_arxiv.pdf\n",
      "choisoonwook.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pdf = os.listdir(\"./data\")\n",
    "for _ in pdf:\n",
    "    print(_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
