{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_data(file_path):\n",
    "    # Creating a PyMuPDFLoader object with file_path\n",
    "    loader = PyMuPDFLoader(file_path=file_path)\n",
    "    \n",
    "    # loading the PDF file\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # returning the loaded document\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responsible for splitting the documents into several chunks\n",
    "def split_docs(documents, chunk_size=1024, chunk_overlap=20):\n",
    "    \n",
    "    # Initializing the RecursiveCharacterTextSplitter with\n",
    "    # chunk_size and chunk_overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    # Splitting the documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    \n",
    "    # returning the document chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading the embedding model\n",
    "def load_embedding_model(model_path, normalize_embedding=True):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs={'device':'cpu'}, # here we will run the model with CPU only\n",
    "        encode_kwargs = {\n",
    "            'normalize_embeddings': normalize_embedding # keep True to compute cosine similarity\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Function for creating embeddings using FAISS\n",
    "def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n",
    "    # Creating the embeddings using FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    # Saving the model in current directory\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    # returning the vectorstore\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### System:\n",
    "You are an AI Assistant that follows instructions extreamly well. \\\n",
    "Help as much as you can.\n",
    "Please say I don't know if you don't know.\n",
    "\n",
    "### User:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### System:\n",
    "You are an respectful and honest assistant. You have to answer the user's \\\n",
    "questions using only the context provided to you. If you don't know the answer, \\\n",
    "just say you don't know. Don't try to make up an answer.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### User:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_chain(retriever, llm, prompt):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever, # here we are using the vectorstore as a retriever\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True, # including source documents in output\n",
    "        chain_type_kwargs={'prompt': prompt} # customizing the prompt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query, chain):\n",
    "    # Getting response from chain\n",
    "    response = chain({'query': query})\n",
    "    \n",
    "    # Wrapping the text for better output in Jupyter Notebook\n",
    "    wrapped_text = textwrap.fill(response['result'], width=100)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choisoonwook/anaconda3/envs/ollama_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Loading llama3:70b from Ollama\n",
    "llm = Ollama(model=\"llama3:70b\", temperature=0)\n",
    "\n",
    "# Loading the Embedding Model\n",
    "embed = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and splitting the documents\n",
    "docs = load_pdf_data(file_path=\"data/choisoonwook.pdf\")\n",
    "documents = split_docs(documents=docs)\n",
    "\n",
    "# creating vectorstore\n",
    "vectorstore = create_embeddings(documents, embed)\n",
    "\n",
    "# converting vectorstore to a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the prompt from the template which we created before\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Creating the chain\n",
    "chain = load_qa_chain(retriever, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choisoonwook/anaconda3/envs/ollama_rag/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document appears to be a personal profile or CV of Soon-Wook Choi, a bachelor student at Korea\n",
      "University's Department of AI Cyber Security. The profile includes:  * Conference presentations (2\n",
      "posters) * Education details (GPA, advisor, and current status) * Research experiences (research\n",
      "student at Artificial Intelligence Cyber Security) * Research interests (quantum machine learning,\n",
      "post-quantum cryptography, etc.) * Projects (3 ongoing projects related to quantum computing, binary\n",
      "code analysis, and network security) * Skills and techniques (version management using Github,\n",
      "Python implementation of CNN codes, and quantum circuit implementation)  Overall, the document\n",
      "showcases Soon-Wook Choi's academic background, research experiences, and skills in the field of AI\n",
      "cyber security.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"Summarize the document\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what QAOA stands for or what it refers to based on the provided context. The context\n",
      "mentions quantum machine learning and quantum circuit implementation, but it does not explicitly\n",
      "mention QAOA. If you could provide more information about QAOA, I may be able to help further.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is QAOA\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what BERT is based on the provided context. The context only provides information about\n",
      "SoonWook Choi's conferences, education, research experiences, interests, projects, and skills, but\n",
      "it does not mention BERT.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is BERT\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soon-Wook Choi is a Bachelor student at Korea University, Department of AI Cyber Security. He is\n",
      "also a Research Student at Artificial Intelligence Cyber Security, Korea University Sejong, Korea.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"Who is SoonWook Choi\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAOA stands for Quantum Approximate Optimization Algorithm. It's a quantum algorithm designed to solve combinatorial optimization problems, which are notoriously difficult to solve exactly in a reasonable amount of time.\n",
      "\n",
      "**The Problem:**\n",
      "Combinatorial optimization problems involve finding the best solution among an exponentially large number of possible solutions. Examples include:\n",
      "\n",
      "1. MaxCut: Divide a graph into two subsets to maximize the number of edges between them.\n",
      "2. Traveling Salesman Problem (TSP): Find the shortest route that visits a set of cities and returns to the starting point.\n",
      "3. Scheduling: Schedule tasks or jobs to minimize delays or maximize efficiency.\n",
      "\n",
      "These problems are NP-hard, meaning that the running time of traditional algorithms increases exponentially with the size of the problem instance.\n",
      "\n",
      "**The Solution:**\n",
      "QAOA is a hybrid quantum-classical algorithm that leverages the power of quantum computing to approximate the solution to these optimization problems. The algorithm consists of three main components:\n",
      "\n",
      "1. **Quantum Circuit**: A parameterized quantum circuit, typically consisting of a sequence of Hadamard gates, phase gates, and entangling gates, which prepares a trial solution.\n",
      "2. **Classical Post-Processing**: A classical optimization algorithm, such as gradient descent or simulated annealing, is used to refine the trial solution obtained from the quantum circuit.\n",
      "3. **Iterative Loop**: The quantum circuit and classical post-processing steps are iteratively applied, with the parameters of the quantum circuit updated based on the previous iteration's results.\n",
      "\n",
      "**How QAOA Works:**\n",
      "\n",
      "1. Initialize the parameters of the quantum circuit.\n",
      "2. Prepare a trial solution using the quantum circuit.\n",
      "3. Measure the trial solution to obtain a classical bitstring.\n",
      "4. Evaluate the quality of the trial solution using a problem-specific objective function.\n",
      "5. Refine the trial solution using classical post-processing.\n",
      "6. Update the parameters of the quantum circuit based on the refined solution.\n",
      "7. Repeat steps 2-6 until convergence or a stopping criterion is reached.\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "1. **Faster Convergence**: QAOA can converge to good solutions faster than classical algorithms for certain problem instances.\n",
      "2. **Improved Scalability**: QAOA can be applied to larger problem sizes, thanks to the power of quantum parallelism.\n",
      "3. **Robustness to Noise**: QAOA is more resilient to noise and errors in the quantum circuit compared to other quantum algorithms.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "1. **Quantum Control Errors**: Maintaining control over the quantum circuit's parameters and mitigating errors remains a significant challenge.\n",
      "2. **Barren Plateaus**: The optimization landscape of QAOA can exhibit barren plateaus, where the algorithm gets stuck in a local optimum.\n",
      "3. **Classical Post-Processing**: The choice of classical post-processing algorithm and its hyperparameters can significantly impact QAOA's performance.\n",
      "\n",
      "QAOA is an active area of research, with ongoing efforts to improve its performance, robustness, and applicability to various problem domains.\n"
     ]
    }
   ],
   "source": [
    "resp1 = llm.invoke(\"What is QAOA\")\n",
    "print(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google in 2018. It's a deep learning model that has revolutionized the field of Natural Language Processing (NLP).\n",
      "\n",
      "**What makes BERT special?**\n",
      "\n",
      "1. **Contextual understanding**: BERT is trained to understand natural language text in context, unlike traditional language models that focus on individual words or phrases.\n",
      "2. **Bidirectional encoding**: BERT uses a bidirectional encoder, which means it considers the entire input sequence (left and right context) when generating representations for each word.\n",
      "3. **Transformer architecture**: BERT is built using the Transformer architecture, introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. This architecture relies on self-attention mechanisms to model relationships between input tokens.\n",
      "\n",
      "**How does BERT work?**\n",
      "\n",
      "1. **Pre-training**: BERT is pre-trained on a large corpus of text data (e.g., Wikipedia, BookCorpus) using two main objectives:\n",
      "\t* **Masked language modeling**: Some input words are randomly replaced with a [MASK] token, and the model predicts the original word.\n",
      "\t* **Next sentence prediction**: The model predicts whether two sentences are adjacent in the original text or not.\n",
      "2. **Fine-tuning**: After pre-training, BERT can be fine-tuned on specific NLP tasks (e.g., sentiment analysis, question-answering, named entity recognition) by adding a task-specific layer on top of the pre-trained model.\n",
      "\n",
      "**What are the benefits of BERT?**\n",
      "\n",
      "1. **State-of-the-art results**: BERT has achieved state-of-the-art results on many NLP benchmarks, often outperforming task-specific models.\n",
      "2. **Transfer learning**: BERT's pre-training allows it to adapt quickly to new tasks with minimal additional training data.\n",
      "3. **Improved interpretability**: BERT's contextualized representations provide a better understanding of language semantics and relationships.\n",
      "\n",
      "**Applications of BERT**\n",
      "\n",
      "1. **Question answering**: BERT has been used for question-answering tasks, such as SQuAD and TriviaQA.\n",
      "2. **Sentiment analysis**: BERT has achieved high accuracy on sentiment analysis tasks, like IMDB and SST-2.\n",
      "3. **Named entity recognition**: BERT has been applied to named entity recognition tasks, such as CoNLL-2003.\n",
      "4. **Text classification**: BERT has been used for text classification tasks, like 20 Newsgroups and Rotten Tomatoes.\n",
      "\n",
      "BERT has had a significant impact on the NLP community, enabling researchers and developers to build more accurate and efficient language models. Its applications continue to expand into various areas, including but not limited to:\n",
      "\n",
      "* Conversational AI\n",
      "* Language translation\n",
      "* Text generation\n",
      "* Dialogue systems\n",
      "\n",
      "I hope this helps you understand what BERT is and its significance in the field of NLP!\n"
     ]
    }
   ],
   "source": [
    "resp2 = llm.invoke(\"What is BERT\")\n",
    "print(resp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soon-Wook Choi is a South Korean professional golfer who has had a successful career in the sport. Here are some key facts about him:\n",
      "\n",
      "**Early Life and Amateur Career**: Born on December 20, 1968, in Seoul, South Korea, Choi started playing golf at the age of 12. He won several amateur tournaments in his home country before turning pro in 1991.\n",
      "\n",
      "**Professional Career**: Choi has won numerous professional tournaments around the world, including:\n",
      "\n",
      "* 2 PGA Tour victories: The 2007 Barclays Classic and the 2011 Players Championship\n",
      "* 4 Japan Golf Tour wins\n",
      "* 3 Asian Tour wins\n",
      "* 2 Korean Tour wins\n",
      "\n",
      "He has also represented South Korea in several international team competitions, such as the Presidents Cup (2003, 2007) and the World Cup (1995, 1996).\n",
      "\n",
      "**Notable Achievements**: Choi's win at the 2011 Players Championship was particularly notable, as it made him the first Asian-born player to win the tournament. He also became the oldest player to win the event at the age of 42.\n",
      "\n",
      "**Style and Impact**: Known for his consistent play and strong iron game, Choi has been an inspiration to many young golfers in South Korea and beyond. His success on the international stage has helped grow the popularity of golf in Asia.\n",
      "\n",
      "Would you like to know more about Soon-Wook Choi's career or achievements?\n"
     ]
    }
   ],
   "source": [
    "resp3 = llm.invoke(\"Who is SoonWook Choi\")\n",
    "print(resp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "A Tutorial on Quantum Approximate Optimization Algorithm QAOA Fundamentals and Applications.pdf\n",
      "self_rag_arxiv.pdf\n",
      "BERT_arxiv.pdf\n",
      "crag_arxiv.pdf\n",
      "RAG_arxiv.pdf\n",
      "choisoonwook.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pdf = os.listdir(\"./data\")\n",
    "for _ in pdf:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78220"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid = os.getpid()\n",
    "pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.kill(pid, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
