{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_data(file_path):\n",
    "    # Creating a PyMuPDFLoader object with file_path\n",
    "    loader = PyMuPDFLoader(file_path=file_path)\n",
    "    \n",
    "    # loading the PDF file\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # returning the loaded document\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responsible for splitting the documents into several chunks\n",
    "def split_docs(documents, chunk_size=1024, chunk_overlap=20):\n",
    "    \n",
    "    # Initializing the RecursiveCharacterTextSplitter with\n",
    "    # chunk_size and chunk_overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    # Splitting the documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    \n",
    "    # returning the document chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading the embedding model\n",
    "def load_embedding_model(model_path, normalize_embedding=True):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs={'device':'cpu'}, # here we will run the model with CPU only\n",
    "        encode_kwargs = {\n",
    "            'normalize_embeddings': normalize_embedding # keep True to compute cosine similarity\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Function for creating embeddings using FAISS\n",
    "def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n",
    "    # Creating the embeddings using FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    # Saving the model in current directory\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    # returning the vectorstore\n",
    "    return vectorstore\n",
    "\n",
    "def load_vector(embedding_model, storing_path=\"vectorstore\"):\n",
    "    vectorstore = FAISS.load_local(storing_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### System:\n",
    "You are an AI Assistant that follows instructions extreamly well. \\\n",
    "Help as much as you can.\n",
    "Please say I don't know if you don't know.\n",
    "\n",
    "### User:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### System:\n",
    "You are an respectful and honest assistant. You have to answer the user's \\\n",
    "questions using only the context provided to you. If you don't know the answer, \\\n",
    "just say you don't know. Don't try to make up an answer.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### User:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_chain(retriever, llm, prompt):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever, # here we are using the vectorstore as a retriever\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True, # including source documents in output\n",
    "        chain_type_kwargs={'prompt': prompt} # customizing the prompt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query, chain):\n",
    "    # Getting response from chain\n",
    "    response = chain({'query': query})\n",
    "    \n",
    "    # Wrapping the text for better output in Jupyter Notebook\n",
    "    wrapped_text = textwrap.fill(response['result'], width=100)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading llama3:70b from Ollama\n",
    "llm = Ollama(model=\"llama3:70b\", temperature=0)\n",
    "\n",
    "# Loading the Embedding Model\n",
    "embed = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_VEC = True\n",
    "vectorstore = None\n",
    "if SAVE_VEC:\n",
    "    # loading and splitting the documents\n",
    "    docs = load_pdf_data(file_path=\"data/QAOA.pdf\")\n",
    "\n",
    "    documents = split_docs(documents=docs)\n",
    "\n",
    "    # creating vectorstore\n",
    "    vectorstore = create_embeddings(documents, embed)\n",
    "else:\n",
    "    vectorstore = load_vector(embed)\n",
    "\n",
    "# converting vectorstore to a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the prompt from the template which we created before\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Creating the chain\n",
    "chain = load_qa_chain(retriever, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document discusses the Quantum Approximate Optimization Algorithm (QAOA), a quantum gate model\n",
      "algorithm that solves optimization problems by mapping the objective function to a Hamiltonian and\n",
      "using quantum mechanical techniques to find the optimal solution. The algorithm has a simple and\n",
      "monotonous structure and relatively good performance. The document also mentions various research\n",
      "topics related to QAOA, including parameter setting, compilation, combinatorial optimization,\n",
      "factoring, and deep learning.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"Summarize the document\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAOA stands for Quantum Alternating Operator Ansatz. It's a quantum gate model algorithm that begins\n",
      "with mapping an objective function to a Hamiltonian, bringing the problem into Hilbert space. Then,\n",
      "it uses quantum mechanical techniques to obtain the expectation value of the Hamiltonian and\n",
      "iteratively finds the parameters that optimize this expectation value.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is QAOA\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what BERT is based on the provided context. The conversation seems to be about Quantum\n",
      "Approximate Optimization Algorithm (QAOA) and its components, but there is no mention of BERT. If\n",
      "you can provide more context or information about what BERT refers to, I'll do my best to help.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is BERT\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The context provided does not mention a person named SoonWook Choi. It only mentions\n",
      "J. Kim as the corresponding author of the paper.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"Who is SoonWook Choi\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAOA stands for Quantum Approximate Optimization Algorithm, (pronounced \"kuh-oh-ah\"). It's a quantum algorithm designed to solve complex optimization problems more efficiently than classical algorithms.\n",
      "\n",
      "**The Problem:**\n",
      "Optimization problems involve finding the best solution among many possible solutions. These problems are ubiquitous in various fields, as they can model real-world scenarios like:\n",
      "\n",
      "* Scheduling tasks\n",
      "* Allocating resources\n",
      "* Portfolio optimization (finance)\n",
      "* Logistics and supply chain management\n",
      "\n",
      "Classical computers struggle to solve these problems efficiently due to their inherent complexity.\n",
      "\n",
      "**The Solution:**\n",
      "QAOA is a hybrid quantum-classical algorithm that leverages the power of quantum computing to approximate optimal solutions. It's designed to tackle complex optimization problems by exploiting the principles of quantum mechanics, such as superposition and entanglement.\n",
      "\n",
      "Here's a high-level overview of how QAOA works:\n",
      "\n",
      "1. **Problem Formulation**: Define the optimization problem using a cost function (or objective function) that needs to be minimized or maximized.\n",
      "2. **Quantum Circuit**: Create a quantum circuit that encodes the problem into a quantum state. This involves applying a series of quantum gates to a set of qubits (quantum bits).\n",
      "3. **Variational Quantum Eigensolver (VQE)**: Use VQE, a hybrid quantum-classical algorithm, to approximate the optimal solution. VQE iteratively updates the quantum circuit parameters to minimize the cost function.\n",
      "4. **Classical Post-processing**: Perform classical post-processing on the output of the quantum circuit to refine the solution.\n",
      "\n",
      "**Key Benefits:**\n",
      "\n",
      "* QAOA can solve complex optimization problems more efficiently than classical algorithms, potentially leading to breakthroughs in various fields.\n",
      "* It's a hybrid algorithm, which means it can be implemented using existing quantum computing hardware and software.\n",
      "* QAOA is relatively easy to implement compared to other quantum algorithms, making it an attractive option for researchers and developers.\n",
      "\n",
      "**Challenges and Limitations:**\n",
      "\n",
      "* QAOA is still an approximate optimization algorithm, meaning it may not always find the global optimum.\n",
      "* The algorithm's performance depends on the quality of the quantum circuit and the classical post-processing techniques used.\n",
      "* Currently, QAOA is limited by the noise and error rates in existing quantum computing hardware.\n",
      "\n",
      "Despite these challenges, QAOA has shown promising results in various applications, including machine learning, logistics, and finance. As quantum computing continues to evolve, we can expect QAOA to play an increasingly important role in solving complex optimization problems.\n"
     ]
    }
   ],
   "source": [
    "resp1 = llm.invoke(\"What is QAOA\")\n",
    "print(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google in  , which has revolutionized the field of Natural Language Processing (NLP).\n",
      "\n",
      "**What does BERT do?**\n",
      "\n",
      "BERT is designed to understand natural language processing tasks, such as:\n",
      "\n",
      "1. **Language Modeling**: Predicting the next word in a sentence based on the context.\n",
      "2. **Text Classification**: Classifying text into categories like spam/not spam, positive/negative sentiment, etc.\n",
      "3. **Named Entity Recognition**: Identifying named entities (people, organizations, locations) in text.\n",
      "4. **Question Answering**: Answering questions based on the content of a passage.\n",
      "\n",
      "**How does BERT work?**\n",
      "\n",
      "BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence. These representations are then fine-tuned for specific NLP tasks.\n",
      "\n",
      "Here's a high-level overview of the architecture:\n",
      "\n",
      "1. **Input Embeddings**: Word embeddings (e.g., WordPiece) and positional embeddings are combined to create input embeddings.\n",
      "2. **Encoder**: A multi-layer bidirectional transformer encoder processes the input embeddings, generating contextualized representations of each word.\n",
      "3. **Pooler**: The final representation of the [CLS] token (a special token added at the beginning of each sentence) is used as the aggregate sequence representation.\n",
      "\n",
      "**What makes BERT so powerful?**\n",
      "\n",
      "1. **Pre-training**: BERT is pre-trained on a large corpus of text data (e.g., Wikipedia, BookCorpus) using a masked language modeling objective. This allows it to learn general language understanding capabilities.\n",
      "2. **Transfer Learning**: Fine-tuning the pre-trained BERT model on specific NLP tasks achieves state-of-the-art results with minimal additional training data and compute resources.\n",
      "3. **Contextualized Representations**: BERT's contextualized representations capture subtle nuances in language, enabling it to understand complex sentences and relationships.\n",
      "\n",
      "**Impact of BERT**\n",
      "\n",
      "BERT has achieved state-of-the-art results on a wide range of NLP tasks, including:\n",
      "\n",
      "1. GLUE benchmark (General Language Understanding Evaluation)\n",
      "2. SQuAD question answering dataset\n",
      "3. IMDB sentiment analysis dataset\n",
      "\n",
      "The success of BERT has led to the development of many variants and extensions, such as RoBERTa, DistilBERT, and ALBERT, which have further improved performance on various NLP tasks.\n",
      "\n",
      "In summary, BERT is a powerful language model that has revolutionized the field of NLP by providing a pre-trained model that can be fine-tuned for specific tasks, achieving state-of-the-art results with minimal additional training data and compute resources.\n"
     ]
    }
   ],
   "source": [
    "resp2 = llm.invoke(\"What is BERT\")\n",
    "print(resp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soon-Wook Choi is a South Korean professional golfer who has had a successful career in the sport. Here are some key facts about him:\n",
      "\n",
      "**Early Life and Amateur Career**: Born on June  , 1963, in Seoul, South Korea, Choi started playing golf at the age of 12. He won several amateur tournaments in his home country before turning pro in 1986.\n",
      "\n",
      "**Professional Career**: Choi has had a prolific career, winning over 20 professional tournaments worldwide, including multiple victories on the Asian Tour, Japanese Tour, and Korean Tour. His most notable win came at the 2003 Korea Open, a co-sanctioned event with the European Tour.\n",
      "\n",
      "**International Appearances**: Choi has represented South Korea in several international team competitions, including the Presidents Cup (1996, 2000), the Dunhill Cup (1989-1991), and the World Cup (1989-1992).\n",
      "\n",
      "**Achievements and Honors**: Choi was named the Asian Tour's Rookie of the Year in 1987 and Player of the Year in 1994. He has also been recognized for his contributions to Korean golf, receiving the Korean Golf Association's highest honor, the \"Golf Merit Award,\" in 2005.\n",
      "\n",
      "**Legacy**: Soon-Wook Choi is considered one of the pioneers of Korean professional golf, paving the way for future generations of Korean golfers. His achievements have inspired many young players in his home country to pursue careers in the sport.\n",
      "\n",
      "Would you like to know more about his career statistics or accomplishments?\n"
     ]
    }
   ],
   "source": [
    "resp3 = llm.invoke(\"Who is SoonWook Choi\")\n",
    "print(resp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "self_rag_arxiv.pdf\n",
      "BERT_arxiv.pdf\n",
      "crag_arxiv.pdf\n",
      "RAG_arxiv.pdf\n",
      "choisoonwook.pdf\n",
      "QAOA.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pdf = os.listdir(\"./data\")\n",
    "for _ in pdf:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25948"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid = os.getpid()\n",
    "pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
