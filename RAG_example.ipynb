{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_data(file_path):\n",
    "    # Creating a PyMuPDFLoader object with file_path\n",
    "    loader = PyMuPDFLoader(file_path=file_path)\n",
    "    \n",
    "    # loading the PDF file\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # returning the loaded document\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responsible for splitting the documents into several chunks\n",
    "def split_docs(documents, chunk_size=1024, chunk_overlap=20):\n",
    "    \n",
    "    # Initializing the RecursiveCharacterTextSplitter with\n",
    "    # chunk_size and chunk_overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    # Splitting the documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    \n",
    "    # returning the document chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading the embedding model\n",
    "def load_embedding_model(model_path, normalize_embedding=True):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs={'device':'cpu'}, # here we will run the model with CPU only\n",
    "        encode_kwargs = {\n",
    "            'normalize_embeddings': normalize_embedding # keep True to compute cosine similarity\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Function for creating embeddings using FAISS\n",
    "def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n",
    "    # Creating the embeddings using FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    # Saving the model in current directory\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    # returning the vectorstore\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### System:\n",
    "You are an AI Assistant that follows instructions extreamly well. \\\n",
    "Help as much as you can.\n",
    "Please say I don't know if you don't know.\n",
    "\n",
    "### User:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### System:\n",
    "You are an respectful and honest assistant. You have to answer the user's \\\n",
    "questions using only the context provided to you. If you don't know the answer, \\\n",
    "just say you don't know. Don't try to make up an answer.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### User:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_chain(retriever, llm, prompt):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever, # here we are using the vectorstore as a retriever\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True, # including source documents in output\n",
    "        chain_type_kwargs={'prompt': prompt} # customizing the prompt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query, chain):\n",
    "    # Getting response from chain\n",
    "    response = chain({'query': query})\n",
    "    \n",
    "    # Wrapping the text for better output in Jupyter Notebook\n",
    "    wrapped_text = textwrap.fill(response['result'], width=100)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading llama3:70b from Ollama\n",
    "llm = Ollama(model=\"llama3:70b\", temperature=0)\n",
    "\n",
    "# Loading the Embedding Model\n",
    "embed = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and splitting the documents\n",
    "docs = load_pdf_data(file_path=\"data/BERT_arxiv.pdf\")\n",
    "documents = split_docs(documents=docs)\n",
    "\n",
    "# creating vectorstore\n",
    "vectorstore = create_embeddings(documents, embed)\n",
    "\n",
    "# converting vectorstore to a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the prompt from the template which we created before\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Creating the chain\n",
    "chain = load_qa_chain(retriever, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document appears to be discussing the fine-tuning of BERT (Bidirectional Encoder Representations\n",
      "from Transformers) on different natural language processing tasks. It provides illustrations of how\n",
      "BERT can be adapted for various tasks, including single-sentence classification, sentence-pair\n",
      "classification, and question-answering.  The document also mentions specific datasets and tasks,\n",
      "such as SST-2 (Stanford Sentiment Treebank), CoLA (Corpus of Linguistic Acceptability), and STS-B.\n",
      "Additionally, it cites several research papers related to BERT and its applications.  Furthermore,\n",
      "the document discusses the results of fine-tuning BERT on a particular task, achieving improved\n",
      "performance compared to baseline systems. It also mentions ablation studies conducted to understand\n",
      "the importance of different facets of BERT.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"Summarize the document\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what QAOA is. The context provided doesn't mention QAOA. It mentions several other\n",
      "acronyms such as MNLI, QQP, QNLI, SQuAD, and NSP, but not QAOA. If you can provide more information\n",
      "or clarify what QAOA stands for, I may be able to help you better.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is QAOA\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model architecture\n",
      "developed by Google that has gained popularity in natural language processing (NLP) tasks. It's a\n",
      "multi-layer bidirectional transformer encoder based on the original implementation described in\n",
      "Vaswani et al. (2017). BERT uses a unified architecture across different tasks and is pre-trained\n",
      "using two unsupervised tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is BERT\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "A Tutorial on Quantum Approximate Optimization Algorithm QAOA Fundamentals and Applications.pdf\n",
      "self_rag_arxiv.pdf\n",
      "BERT_arxiv.pdf\n",
      "crag_arxiv.pdf\n",
      "RAG_arxiv.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pdf = os.listdir(\"./data\")\n",
    "for _ in pdf:\n",
    "    print(_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
